// src/lib/ai-config.ts
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import { streamText } from 'ai';

// ============================================================================
// ZENTRALE AI-KONFIGURATION
// ============================================================================

export const AI_CONFIG = {
  // Modell-Kette: Beste zuerst, dann Fallbacks
  models: [
    'gemini-3-flash-preview',  // Primary (Free Tier, beste Qualit√§t, aber 20 Req/Tag Limit)
    'gemini-2.5-flash',        // Fallback 1 (Pay-as-you-go verf√ºgbar)
    'gemini-2.0-flash',        // Fallback 2 (g√ºnstigster Paid, sehr stabil)
  ] as const,
  
  // Shortcuts f√ºr direkten Zugriff (R√ºckw√§rtskompatibilit√§t)
  primaryModel: 'gemini-flash-latest' as const,
  fallbackModel: 'gemini-2.5-flash' as const,
  lastResortModel: 'gemini-2.0-flash' as const,
  
  // Temperature-Presets
  settings: {
    strict: { temperature: 0.1 },    // F√ºr JSON, Code, Fakten
    balanced: { temperature: 0.7 },  // Standard
    creative: { temperature: 0.9 },  // F√ºr Content, Brainstorming
  },
  
  // Default Temperature
  temperature: 0.7,
};

// ============================================================================
// GOOGLE AI PROVIDER
// ============================================================================

const google = createGoogleGenerativeAI({
  apiKey: process.env.GEMINI_API_KEY || '',
});

export { google };

// ============================================================================
// TYPEN
// ============================================================================

type ModelStatus = 'primary' | 'fallback' | 'lastResort';

// Metadata die wir zum Result hinzuf√ºgen
interface AIMetadata {
  _modelName: string;
  _status: ModelStatus;
}

// Typ f√ºr das erweiterte Result (StreamTextResult + unsere Metadata)
type EnhancedStreamResult = Awaited<ReturnType<typeof streamText>> & AIMetadata;

// ============================================================================
// HELPER: Rate-Limit-Erkennung
// ============================================================================

function isRateLimitError(error: unknown): boolean {
  const errorStr = String(error).toLowerCase();
  return (
    errorStr.includes('429') ||
    errorStr.includes('quota') ||
    errorStr.includes('rate limit') ||
    errorStr.includes('too many requests') ||
    errorStr.includes('resource exhausted') ||
    errorStr.includes('exceeded')
  );
}

function getErrorType(error: unknown): 'rateLimit' | 'serverError' | 'unknown' {
  if (isRateLimitError(error)) return 'rateLimit';
  
  const errorStr = String(error).toLowerCase();
  if (errorStr.includes('500') || errorStr.includes('503') || errorStr.includes('internal')) {
    return 'serverError';
  }
  
  return 'unknown';
}

// ============================================================================
// HAUPTFUNKTION: streamTextSafe mit Multi-Fallback
// ============================================================================

/**
 * F√ºhrt streamText mit automatischem Multi-Fallback aus.
 * Versucht alle Modelle in der Reihenfolge: Primary ‚Üí Fallback ‚Üí LastResort
 * 
 * R√úCKW√ÑRTSKOMPATIBEL: Gibt direkt das StreamTextResult zur√ºck,
 * mit zus√§tzlichen Properties _modelName und _status.
 * 
 * @example
 * const result = await streamTextSafe({
 *   prompt: 'Analysiere diese Daten...',
 *   temperature: 0.3,
 * });
 * return result.toTextStreamResponse(); // Funktioniert direkt!
 */
export async function streamTextSafe(
  params: Omit<Parameters<typeof streamText>[0], 'model'>
): Promise<EnhancedStreamResult> {
  
  const models = AI_CONFIG.models;
  const statusMap: Record<number, ModelStatus> = {
    0: 'primary',
    1: 'fallback', 
    2: 'lastResort'
  };
  
  let lastError: Error | null = null;

  for (let i = 0; i < models.length; i++) {
    const modelName = models[i];
    const status = statusMap[i] || 'lastResort';
    
    try {
      // Logging nur bei Fallback
      if (i === 0) {
        console.log(`ü§ñ AI-Manager: Starte mit ${modelName}`);
      } else {
        console.log(`üîÑ AI-Manager: Fallback auf ${modelName}...`);
      }
      
      const result = await streamText({
        ...params,
        model: google(modelName),
      } as any);

      // Erfolg!
      if (i > 0) {
        console.log(`‚úÖ AI-Manager: ${modelName} erfolgreich (nach ${i} Fallback${i > 1 ? 's' : ''})`);
      }
      
      // F√ºge Metadata zum Result hinzu (f√ºr optionales Tracking)
      // Cast √ºber unknown n√∂tig, da wir Properties zu einem bestehenden Objekt hinzuf√ºgen
      const enhancedResult = result as unknown as EnhancedStreamResult;
      enhancedResult._modelName = modelName;
      enhancedResult._status = status;
      
      return enhancedResult;
      
    } catch (error) {
      lastError = error as Error;
      const errorType = getErrorType(error);
      
      // Detailliertes Logging
      if (errorType === 'rateLimit') {
        console.warn(`‚è≥ AI-Manager: Rate Limit bei ${modelName}`);
      } else if (errorType === 'serverError') {
        console.warn(`üî• AI-Manager: Server Error bei ${modelName}`);
      } else {
        console.warn(`‚ö†Ô∏è AI-Manager: ${modelName} fehlgeschlagen:`, error);
      }
    }
  }

  // Alle Modelle fehlgeschlagen
  const finalError = new Error(
    `Alle AI-Modelle fehlgeschlagen (${models.join(' ‚Üí ')}). ` +
    `Letzter Fehler: ${lastError?.message || 'Unbekannt'}`
  );
  
  console.error('‚ùå AI-Manager: Alle Modelle ersch√∂pft!', finalError);
  throw finalError;
}

// ============================================================================
// CONVENIENCE: Response mit Headers erstellen
// ============================================================================

/**
 * Erstellt eine Text Stream Response mit AI-Status-Headers
 * Optional - kann verwendet werden, um Model-Info im Response-Header zu haben
 * 
 * @example
 * const result = await streamTextSafe({ prompt: '...' });
 * return createAIStreamResponse(result);
 */
export function createAIStreamResponse(
  result: EnhancedStreamResult,
  additionalHeaders?: Record<string, string>
): Response {
  return result.toTextStreamResponse({
    headers: {
      'X-AI-Model': result._modelName || 'unknown',
      'X-AI-Status': result._status || 'unknown',
      ...additionalHeaders,
    },
  });
}
